{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOWfRgoZDrsn"
      },
      "source": [
        "# Neural Net Lab\n",
        "- Student ID: 19120691\n",
        "- Student name: Lê Quốc Trí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eazZSw1Drsq"
      },
      "source": [
        "---\n",
        "\n",
        "You will work directly on this notebook; the word `TODO` indicates the part you need to do.  You can discuss ideas with classmates as well as find information from the internet, book, ... but at the end *the code as well as writing in this lab must be your, based on your own understanding*.\n",
        "\n",
        "Before submitting, rerun the notebook (`Runtime` ->` Restart and run all`). Then download the notebook file (`File` -> `Download` -> `Download .ipynb`). Then create a folder named `ID` (for example, if your ID is 1234567, create a folder named `1234567`), copy the notebook file to the folder, compress the folder to .zip file and submit .zip file on moodle.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBqNCqtaMGQS"
      },
      "source": [
        "**What problem do we want to solve in this lab?**\n",
        "\n",
        "Given the training data: \n",
        "$$\\{(\\textbf{x}^{(1)}, y^{(1)}), ..., (\\textbf{x}^{(N)}, y^{(N)})\\}$$\n",
        "where:\n",
        "\n",
        "- $\\textbf{x}^{(n)} \\in \\mathbb{R}^{784}$ is an input vector containing pixel values of a $28 \\times 28$ grayscale image of some hand-written digit\n",
        "- $y^{(n)} \\in \\{0, 1, 2, ..., 9\\}$ is the corresponding output indicating which digit\n",
        "\n",
        "Our task is building a model (in this lab: a Fully-Connected Feed-Forward Neural Net) from this data so that it can take a *new* hand-written digit image (a vector $\\in \\mathbb{R}^{784}$) as input and predict the output (which digit) *well*.\n",
        "\n",
        "Before starting, you should review materials about Neural Net (note that different materials can use different notations, but the essence is the same):\n",
        "- [The course's Neural Net slides](https://drive.google.com/drive/folders/1KDifaACbAuomUiyBsiZfN09gFoKa5iqo) (lect12)\n",
        "- [3B1B's Neural Net videos](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) (I think it's beautiful and inspirational)\n",
        "- [My Neural Net slides](https://drive.google.com/file/d/1IH7nXTR-0kVxWyyDlNorlxBAyl0eWECb/view?usp=sharing) (Probably it's the most relevant to this lab)\n",
        "\n",
        "Are you ready? Let's start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyGk-tVIMGQT"
      },
      "source": [
        "## Import Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TBxvVfg0MGQU"
      },
      "outputs": [],
      "source": [
        "# You will use Numpy as main lib in this lab\n",
        "# So, you should use Numpy operations on Numpy arrays, avoid using loops;\n",
        "# otherwise, the code will run slow\n",
        "import numpy as np \n",
        "\n",
        "# Import other libs\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# We will not use anything else"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download data"
      ],
      "metadata": {
        "id": "YM4TkCZeUCi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific data we will use in this lab is the famous MNIST dataset of hand-written digit images. The command below will download this data and save as \"mnist.pkl.gz\" file to the directory containing this notebook in Google Colab machine."
      ],
      "metadata": {
        "id": "LGic0EdjhLua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=1D_D5UWoPvyUDoTxwJ2fiRKEh7cYSsPwV\" -O \"mnist.pkl.gz\""
      ],
      "metadata": {
        "id": "A9ReqTOOUGyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6bd35da-77fd-42d6-d57e-9cffed707e04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-05 08:20:00--  https://drive.google.com/uc?export=download&id=1D_D5UWoPvyUDoTxwJ2fiRKEh7cYSsPwV\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.216.100, 173.194.216.101, 173.194.216.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.216.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4n4j0qlk9dkcr4d8qvp2bu8hr85h8107/1654417200000/15182747727986447439/*/1D_D5UWoPvyUDoTxwJ2fiRKEh7cYSsPwV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-06-05 08:20:02--  https://doc-14-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4n4j0qlk9dkcr4d8qvp2bu8hr85h8107/1654417200000/15182747727986447439/*/1D_D5UWoPvyUDoTxwJ2fiRKEh7cYSsPwV?e=download\n",
            "Resolving doc-14-0g-docs.googleusercontent.com (doc-14-0g-docs.googleusercontent.com)... 108.177.12.132, 2607:f8b0:400c:c08::84\n",
            "Connecting to doc-14-0g-docs.googleusercontent.com (doc-14-0g-docs.googleusercontent.com)|108.177.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16168813 (15M) [application/gzip]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "mnist.pkl.gz        100%[===================>]  15.42M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-06-05 08:20:02 (131 MB/s) - ‘mnist.pkl.gz’ saved [16168813/16168813]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "hfkPC9y4dw0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07aea3a-05e1-44c4-d665-d44ce7d63b37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mnist.pkl.gz  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nLwF1PtMGQW"
      },
      "source": [
        "## Read data and explore a little bit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[The original MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains 2 sets: training set (60000 images) and test set (10000 images). The MNIST dataset we will use in this lab (the \"mnist.pkl.gz\" file) contains 3 sets: training set (50000 images), validation set (10000 images), and test set (10000 images); the training set and the validation set here were created by splitting the original training set (60000 images).  \n",
        "\n",
        "The code below will read data from the \"mnist.pkl.gz\" file and store data in 6 Numpy arrays:\n",
        "\n",
        "- `train_X`, `train_Y`\n",
        "- `val_X`, `val_Y`\n",
        "- `test_X`, `test_Y`"
      ],
      "metadata": {
        "id": "-kw93HG1f8cr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Iu5JorB7MGQY"
      },
      "outputs": [],
      "source": [
        "def read_mnist(mnist_file):\n",
        "    '''\n",
        "    Reads MNIST data.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    mnist_file : string\n",
        "        The name of the MNIST file (e.g., 'mnist.pkl.gz').\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (train_X, train_Y, val_X, val_Y, test_X, test_Y) : tuple\n",
        "        train_X : numpy array, shape (N=50000, d=784)\n",
        "            Input vectors of the training set.\n",
        "        train_Y : numpy array, shape (N=50000)\n",
        "            Outputs of the training set.\n",
        "        val_X : numpy array, shape (N=10000, d=784)\n",
        "            Input vectors of the validation set.\n",
        "        val_Y : numpy array, shape (N=10000)\n",
        "            Outputs of the validation set.\n",
        "        test_X : numpy array, shape (N=10000, d=784)\n",
        "            Input vectors of the test set.\n",
        "        test_Y : numpy array, shape (N=10000)\n",
        "            Outputs of the test set.\n",
        "    '''\n",
        "    \n",
        "    f = gzip.open(mnist_file, 'rb')\n",
        "    train_data, val_data, test_data = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    \n",
        "    train_X, train_Y = train_data\n",
        "    val_X, val_Y = val_data\n",
        "    test_X, test_Y = test_data    \n",
        "    \n",
        "    return train_X, train_Y, val_X, val_Y, test_X, test_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rJBRsk8GMGQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951e3c95-6c14-40d5-eea6-4a321195b65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X.shape = (50000, 784)\n",
            "train_Y.shape = (50000,)\n",
            "val_X.shape   = (10000, 784)\n",
            "val_Y.shape   = (10000,)\n",
            "test_X.shape  = (10000, 784)\n",
            "test_Y.shape  = (10000,)\n",
            "\n",
            "train_X: min = 0.000, max = 0.996\n",
            "train_Y: min = 0, max = 9\n"
          ]
        }
      ],
      "source": [
        "train_X, train_Y, val_X, val_Y, test_X, test_Y = read_mnist('mnist.pkl.gz')\n",
        "\n",
        "print('train_X.shape =', train_X.shape)\n",
        "print('train_Y.shape =', train_Y.shape)\n",
        "print('val_X.shape   =', val_X.shape)\n",
        "print('val_Y.shape   =', val_Y.shape)\n",
        "print('test_X.shape  =', test_X.shape)\n",
        "print('test_Y.shape  =', test_Y.shape)\n",
        "\n",
        "print('\\ntrain_X: min = %.3f, max = %.3f' %(train_X.min(), train_X.max()))\n",
        "print('train_Y: min = %d, max = %d' %(train_Y.min(), train_Y.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsDjR6u6Drst"
      },
      "source": [
        "## Build a good Neural Net from data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCw8g8LpMGQb"
      },
      "source": [
        "### Implement functions to train a Neural Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3mEfVqIMGQb"
      },
      "source": [
        "In this lab, we will use a simple form of Neural Net: Fully-Connected Feed-Forward Neural Net. We will use sigmoid function as activation function in hidden layers *as well as in output layer* (using sigmoid instead of softmax in output layer will make our life easier). We will have $K$ neurons in output layer with $K$ is the number of classes (in our case, $K = 10$); the output of each neuron will indicate the probability a given input vector belonging to the corresponding class (note that sum of all these probabilities will not equal 1 as softmax). We can choose the class with highest probability as the predicted class for a given input vector.\n",
        "\n",
        "\n",
        "In our first step, we will implement `compute_nnet_outputs` function *(3 points)*. We will use this function not only after training (to compute outputs of our trained Neural Net with new input vectors) but also during the training process (which we will tackle in our next step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g59_mKA0MGQc"
      },
      "outputs": [],
      "source": [
        "def sigmoid(S):\n",
        "    '''\n",
        "    Computes sigmoid function for each element of array S.\n",
        "    You can use this function in `compute_nnet_outputs` function.\n",
        "    '''\n",
        "    return 1 / (1 + np.exp(-S))\n",
        "\n",
        "def compute_nnet_outputs(Ws, X, need_all_layer_outputs):\n",
        "    '''\n",
        "    Computes the outputs of Neural Net by forward propagating X through the net.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Ws : list of numpy arrays\n",
        "        Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer, \n",
        "        it doesn't have W); W of layer l will have the shape of \n",
        "        (d^(l-1)+1, d^(l)), where  d^(l-1) is the number of neurons \n",
        "        (not count the +1 neuron) of layer l-1 and  d^(l) is the number of \n",
        "        neurons (not count the +1 neuron) of layer l.\n",
        "    X : numpy array, shape (N, d+1)\n",
        "        The matrix of input vectors (each row corresponds to an input vector); \n",
        "        the first column of this matrix is all ones (corresponding to x_0).\n",
        "    need_all_layer_outputs : bool\n",
        "        If this var is true, we'll return a list of layer's-outputs (we'll \n",
        "        need this list when training); otherwise, we'll return the final \n",
        "        layer's output.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    If `need_all_layer_outputs` is false, return\n",
        "        A : numpy array, shape (N, K=10)\n",
        "            The maxtrix of output vectors of final layer; each row is an \n",
        "            output vector (containing each class's probability given the \n",
        "            corresponding input vector).\n",
        "    Else, return\n",
        "        As : list of numpy arrays\n",
        "            As[l] is the matrix of output vectors of layer l (l=0 will \n",
        "            correspond to input layer); each row is an output vector \n",
        "            (corresponding to an input vector); if layer l is not the final \n",
        "            layer, the first column of this matrix is all ones.\n",
        "    '''    \n",
        "    \n",
        "    # TODO\n",
        "    # layer_outputs chứa output của các layer, riêng layer đầu là các input vectors\n",
        "    # Vòng lặp for để tính các output của các layers và thêm vào list\n",
        "    #   Nếu yêu cầu output của tất cả các layer thì thêm output của layer cuối vào list và return list\n",
        "    layer_outputs = [X]\n",
        "    for i in range(len(Ws)-1):\n",
        "      X = sigmoid(X@Ws[i])\n",
        "      X = np.hstack((np.ones((X.shape[0],1)),X)) #Thêm cột hằng số 1+ vào bên trái ma trận output của layer trước đó\n",
        "      layer_outputs.append(X)\n",
        "    last_output = sigmoid(X@Ws[-1])\n",
        "    \n",
        "    if not need_all_layer_outputs:\n",
        "      return last_output\n",
        "    layer_outputs.append(last_output)\n",
        "    return layer_outputs\n",
        "    \n",
        "    # NOTE: to make the code run fast, you should use Numpy operations on\n",
        "    # Numpy arrays; your code should have only one loop to loop through layers\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5HfhG-nEMGQc"
      },
      "outputs": [],
      "source": [
        "# CHECK THE CORRECTNESS OF YOUR COMPUTE_NNET_OUTPUTS FUNCTION\n",
        "\n",
        "# A small X with 4 rows corresponding to 4 input vectors\n",
        "X = np.array([[1.0, 0.9, 0.9], \n",
        "              [1.0, 0.5, 0.4], \n",
        "              [1.0, 0.4, 0.5],\n",
        "              [1.0, 0.1, 0.7]])\n",
        "\n",
        "# A small neural net: \n",
        "# 2 input neurons - 3 hidden neurons - 2 hidden neurons - 1 output neurons\n",
        "# (not counting +1 neurons)\n",
        "Ws = [np.array([[-0.3 ,  0.2 ,  0.5 ],\n",
        "                [-0.1 , -0.2 , -0.35],\n",
        "                [ 0.45, -0.7 , -0.7 ]]),\n",
        "      np.array([[ 0.3 , -0.05],\n",
        "                [ 0.6 ,  0.3 ],\n",
        "                [-0.8 , -0.3 ],\n",
        "                [ 0.4 , -0.45]]),\n",
        "      np.array([[-0.3 ],\n",
        "                [ 0.5 ],\n",
        "                [-0.45]])]\n",
        "\n",
        "# Check your compute_nnet_outputs function!\n",
        "A = compute_nnet_outputs(Ws, X, False)\n",
        "assert np.array_equal(np.round(A, 5),\n",
        "                      np.array([[0.45109],\n",
        "                                [0.45199],\n",
        "                                [0.4521 ],\n",
        "                                [0.45247]]))\n",
        "As = compute_nnet_outputs(Ws, X, True)\n",
        "assert len(As) == 4\n",
        "assert np.array_equal(np.round(As[0], 5),\n",
        "                      np.array([[1. , 0.9, 0.9],\n",
        "                                [1. , 0.5, 0.4],\n",
        "                                [1. , 0.4, 0.5],\n",
        "                                [1. , 0.1, 0.7]]))\n",
        "assert np.array_equal(np.round(As[1], 5),\n",
        "                      np.array([[1.     , 0.50375, 0.35206, 0.39055],\n",
        "                                [1.     , 0.4576 , 0.45512, 0.51125],\n",
        "                                [1.     , 0.47128, 0.44275, 0.5025 ],\n",
        "                                [1.     , 0.50125, 0.42311, 0.49375]]))\n",
        "assert np.array_equal(np.round(As[2], 5),\n",
        "                      np.array([[1.     , 0.617  , 0.45506],\n",
        "                                [1.     , 0.60228, 0.43062],\n",
        "                                [1.     , 0.60577, 0.4335 ],\n",
        "                                [1.     , 0.61296, 0.43813]]))\n",
        "assert np.array_equal(np.round(As[3], 5),\n",
        "                      np.array([[0.45109],\n",
        "                                [0.45199],\n",
        "                                [0.4521 ],\n",
        "                                [0.45247]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWpqr_8WMGQd"
      },
      "source": [
        "Now, we will implement our main function: `train_nnet` *(4 points)*. Basically, in this function, we will find values of our Neural Net weights by minimizing mean cross-entropy error on the training set  $\\{(\\textbf{x}^{(1)}, y^{(1)}), ..., (\\textbf{x}^{(N)}, y^{(N)})\\}$:\n",
        "$$E(w\\text{'s of our Net}) = \\frac{1}{N} \\sum_{n=1}^Ne(h(\\textbf{x}^{(n)}), \\textbf{y}^{(n)}))$$\n",
        "where:\n",
        "- $h(\\textbf{x}^{(n)}) \\in \\mathbb{R}^{K}$ is the output vector (containing probabilities) of our Neural Net corresponding to the input vector $\\textbf{x}^{(n)}$\n",
        "- $\\textbf{y}^{(n)} \\in \\mathbb{R}^{K}$ is the one-hot representation of $y^{(n)}$ (can you see the difference between  $\\textbf{y}^{(n)}$ and $y^{(n)}$?) \\\n",
        "For example, in our case $K=10$ (10 digits):\n",
        "    - If $y^{(n)}=0$ (digit 0) then $\\textbf{y}^{(n)}=[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]^T$\n",
        "    - If $y^{(n)}=1$ (digit 1) then $\\textbf{y}^{(n)}=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]^T$\n",
        "- $e(h(\\textbf{x}^{(n)}), \\textbf{y}^{(n)}))$ is the cross-entropy error on a single training pair $(\\textbf{x}^{(n)}, y^{(n)})$:\n",
        "$$e(h(\\textbf{x}^{(n)}), \\textbf{y}^{(n)})) = \\sum_{k=1}^K - y^{(n)}_k \\ln h(\\textbf{x}^{(n)})_k - (1 - y^{(n)}_k)\\ln (1 - h(\\textbf{x}^{(n)})_k)$$\n",
        "\n",
        "If we use GD (Gradient Descent) to minimize $E(w\\text{'s of our Net})$, we will need to compute partial derivatives of $E$ with respect to $w\\text{'s}$. To compute these partial derivatives, we will compute partial derivatives of $e$ (cross-entropy error on a single training pair $\\mathbf{x}$ and $y$) with respect to $w\\text{'s}$, and then take average over all training pairs. But it will slow when our training set is big. SGD (Stochastic Gradient Descent) approximates the average of partial derivatives over all training pairs by the average of partial derivatives over a mini-batch (a subset of training pairs); so, it's faster than GD. Another name of SGD: \"túy quyền\" ;-). In `train_nnet` function below, we will use SGD.\n",
        "\n",
        "Let's focus on how to compute the partial derivative of $e$ (cross-entropy error on a single training pair $\\mathbf{x}$ and $y$) with respect to a weight $w$ between a neuron i in layer l-1 and a neuron j in layer l. After applying chain rule (see [my Neural Net slides](https://drive.google.com/file/d/1IH7nXTR-0kVxWyyDlNorlxBAyl0eWECb/view) for details), we have the formula: \n",
        "$$\\text{this partial derivative} = \\text{output of neuron i in layer l-1} \\times \\text{delta of neuron j in layer l}$$\n",
        "with delta of a neuron is the partial derivative of $e$ with respect to the weighted sum (value before applying activation function) of that neuron.\n",
        "\n",
        "I have implemented most of `train_nnet` function for you. Your job is to write 3 lines of code corresponding to 3/4 tasks below:\n",
        "\n",
        "**Task 1. Compute delta's of the last layer (on a mini-batch)**\n",
        "\n",
        "For example, let's consider $K=3$ (3 classes) and a mini-batch with size of 2 (2 training pairs), if:\n",
        "\n",
        "        A = np.array([[0.8, 0.7, 0.6],\n",
        "                      [0.5, 0.6, 0.5]]) # Output vectors of last layer \n",
        "                                          on this mini-batch\n",
        "        mb_Y = np.array([[0, 1, 0],\n",
        "                         [1, 0, 0]]) # Correct one-hot output vectors \n",
        "                                       of this mini-batch\n",
        "then delta's of the last layer on this mini-batch will be:\n",
        "\n",
        "        delta = np.array([[ 0.8, -0.3,  0.6],\n",
        "                          [-0.5,  0.6,  0.5]])\n",
        "You should do the math to figure out the exact formula to compute delta's of the last layer. In case it's difficult for you, try your luck and guess the formula from the result above ;-). Again, when implementing this task, you should use Numpy operations on Numpy arrays; this task should be done with just one line of code.\n",
        "\n",
        "**Task 2. Compute gradient - a collection of partial derivatives - of the last layer from delta's of the last layer and outputs of the previous layer (on a mini-batch)**\n",
        "\n",
        "For example, let's consider $K=3$ (3 classes; it's also the number of neurons in the last layer L), the previous layer (layer L-1) with 4 neurons, and a mini-batch with size of 2 (2 training pairs), if:\n",
        "\n",
        "        delta = np.array([[ 0.8, -0.3,  0.6],\n",
        "                          [-0.5,  0.6,  0.5]]) # Delta vectors of last layer \n",
        "                                                 on this mini-batch\n",
        "        A = np.array([[1.0, 0.5, 0.1, 0.3, 0.2],\n",
        "                      [1.0, 0.9, 0.8, 0.7, 0.1]]) # Output vectors of the previous layer \n",
        "                                                    on this mini-batch\n",
        "then the gradient of the last layer will be (it has the same shape with `W` array of the last layer):\n",
        "\n",
        "        grad = np.array([[ 0.15 ,  0.15 ,  0.55 ],\n",
        "                         [-0.025,  0.195,  0.375],\n",
        "                         [-0.16 ,  0.225,  0.23 ],\n",
        "                         [-0.055,  0.165,  0.265],\n",
        "                         [ 0.055,  0.   ,  0.085]]) # For clarity, I use \n",
        "                                                      np.round(..., 3) to show this\n",
        "How is `grad` computed from `delta` and `A`? Hint: I have mentioned above how to compute the partial derivative of cross-entropy error on a single training pair with respect to a weight; for a mini-batch, just compute it for each training pair and then take average.\n",
        "\n",
        "Again, when implementing this task, you should use Numpy operations on Numpy arrays; this task should be done with just one line of code.\n",
        "\n",
        "**Task 3. Compute delta's of layer l from delta's of layer l+1 (on a mini-batch)**\n",
        "\n",
        "(Note that we will not compute delta's of +1 neurons)\n",
        "\n",
        "This is the most difficult task, but I have done it for you :-). \n",
        "\n",
        "\n",
        "**Task 4. Compute gradient of layer l from delta of layer l and outputs of layer l-1 (on a mini-batch)**\n",
        "\n",
        "It's similar to task 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I1X9X0MwMGQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c351b11-a7ec-4430-af09-838569ab6fa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.15 ,  0.15 ,  0.55 ],\n",
              "       [-0.025,  0.195,  0.375],\n",
              "       [-0.16 ,  0.225,  0.23 ],\n",
              "       [-0.055,  0.165,  0.265],\n",
              "       [ 0.055,  0.   ,  0.085]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "delta = np.array([[ 0.8, -0.3,  0.6],\n",
        "                      [-0.5,  0.6,  0.5]])\n",
        "A = np.array([[1.0, 0.5, 0.1, 0.3, 0.2],\n",
        "                  [1.0, 0.9, 0.8, 0.7, 0.1]])\n",
        "np.round(A.T.dot(delta) / 2, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pyWfj_vrMGQh"
      },
      "outputs": [],
      "source": [
        "def train_nnet(X, Y, val_X, val_Y, \n",
        "               hid_layer_sizes, \n",
        "               mb_size, learning_rate, max_epoch):\n",
        "    '''\n",
        "    Trains Neural Net on the dataset (X, Y); also prints out mean binary error \n",
        "    (the percentage of misclassified data points) on training set and \n",
        "    validation set during training\n",
        "    Cost function: mean cross-entropy error\n",
        "    Optimization algorithm: SGD\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : numpy array, shape (N, d + 1)\n",
        "        The matrix of input vectors (each row corresponds to an input vector); \n",
        "        the first column of this matrix is all ones (corresponding to x_0).\n",
        "    Y : numpy array, shape (N,) \n",
        "        The vector of outputs.\n",
        "    val_X, val_Y : validation data, similar to X and Y\n",
        "    hid_layer_sizes : list\n",
        "        The list of hidden layer sizes; e.g., hid_layer_sizes = [20, 10] means\n",
        "        the Net has 2 hidden layers, the 1st one has 20 neurons, and the 2nd \n",
        "        one has 10 neurons (not count the +1 neurons).\n",
        "    mb_size : int\n",
        "        Minibatch size of SGD.\n",
        "    learning_rate : float\n",
        "        Learning rate of SGD.\n",
        "    max_epoch : int\n",
        "        After this number of epochs, we'll terminate SGD.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (Ws, costs, errs) : tuple\n",
        "        Ws : list of numpy arrays\n",
        "            Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer, \n",
        "            it doesn't have W); W of layer l will have the shape of \n",
        "            (d^(l-1)+1, d^(l)), where d^(l-1) is the number of neurons \n",
        "            (not count the +1 neuron) of layer l-1 and d^(l) is the number of \n",
        "            neurons (not count the +1 neuron) of layer l.\n",
        "        costs : list, len = max_epoch\n",
        "            The list of costs after each epoch.\n",
        "        errs : list, len = max_epoch\n",
        "            The list of mean binary errors (on the training set) after each epoch.\n",
        "    '''\n",
        "    \n",
        "    # Prepare for training\n",
        "    K = len(np.unique(Y)) # Num classes\n",
        "    layer_sizes = [X.shape[1] - 1] + hid_layer_sizes + [K]\n",
        "    np.random.seed(0) # This will fix the randomization; \n",
        "                      # so, you and me will have the same results\n",
        "    Ws = [np.random.randn(layer_sizes[i] + 1, layer_sizes[i + 1]) \n",
        "          / np.sqrt(layer_sizes[i] + 1) \n",
        "          for i in range(len(layer_sizes) - 1)] # Secret formula to init Ws ;-)\n",
        "    one_hot_Y = np.zeros((len(Y), K))\n",
        "    one_hot_Y[np.arange(len(Y)), Y] = 1\n",
        "    errs = [] # To save mean binary errors on training set during training\n",
        "    val_errs = [] # To save mean binary errors on validation set during training\n",
        "    N = len(X) # Num training pairs\n",
        "    rnd_idxs = np.arange(N) # Random indexes    \n",
        "    \n",
        "    # Train\n",
        "    for epoch in range(max_epoch):\n",
        "        np.random.shuffle(rnd_idxs)\n",
        "        for start_idx in range(0, N, mb_size):\n",
        "            # Get minibach\n",
        "            mb_X = X[rnd_idxs[start_idx:start_idx+mb_size]]\n",
        "            mb_Y = one_hot_Y[rnd_idxs[start_idx:start_idx+mb_size]]\n",
        "            \n",
        "            # Forward-prop\n",
        "            As = compute_nnet_outputs(Ws, mb_X, True)\n",
        "            \n",
        "            # Back-prop; on the way, compute each layer's gradient and update its W\n",
        "            # TODO: delta = ... (task 1)\n",
        "            delta = As[-1] - mb_Y\n",
        "\n",
        "            # TODO: grad = ... (task 2)\n",
        "            grad = As[-2].T@delta\n",
        "            mbsize = np.where(start_idx+mb_size <= N, mb_size, N%mb_size) # Tìm kích thước mini batch cuối cùng\n",
        "            grad /= mbsize\n",
        "            # Notes on task 2: \n",
        "            # if N % mb_size != 0 \n",
        "            # => last minibatch's size < mb_size\n",
        "            # => when computing grad for last minibatch, \n",
        "            #    we need to divide by last minibatch's size instead of mb_size\n",
        "            \n",
        "            Ws[-1] -= learning_rate * grad\n",
        "            for i in range(2, len(Ws) + 1):\n",
        "                delta = delta.dot(Ws[-i + 1].T[:, 1:]) * As[-i][:, 1:] * (1 - As[-i][:, 1:])\n",
        "                # TODO: grad = ... (task 4)\n",
        "                # Notes on task 4: similar to notes on task 2\n",
        "                grad = As[-i-1].T@delta/mbsize\n",
        "                Ws[-i] -= learning_rate * grad\n",
        "        \n",
        "        # Compute training info, save it, and print it\n",
        "        A = compute_nnet_outputs(Ws, X, False)\n",
        "        err = np.mean(np.argmax(A, axis=1) != Y) * 100\n",
        "        errs.append(err)\n",
        "        val_A = compute_nnet_outputs(Ws, val_X, False)\n",
        "        val_err = np.mean(np.argmax(val_A, axis=1) != val_Y) * 100\n",
        "        val_errs.append(val_err)\n",
        "        print('Epoch %d, train err %.3f%%, val err %.3f%%' %(epoch, err, val_err))\n",
        "            \n",
        "    return Ws, errs, val_errs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SpWzYY3oMGQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52414e27-bb85-4a72-dd07-29bf3f618fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train err 10.982%, val err 9.820%\n",
            "Epoch 1, train err 9.044%, val err 8.230%\n",
            "Epoch 2, train err 7.962%, val err 7.490%\n"
          ]
        }
      ],
      "source": [
        "# CHECK THE CORRECTNESS YOUR TRAIN_NNET FUCTION\n",
        "Ws, train_errs, val_errs = train_nnet(train_X, train_Y, val_X, val_Y,\n",
        "                                      hid_layer_sizes=[20], \n",
        "                                      mb_size=32, learning_rate=0.1, max_epoch=3)\n",
        "assert np.array_equal(np.round(train_errs, 3),\n",
        "                      np.array([10.982, 9.044, 7.962]))\n",
        "assert np.array_equal(np.round(val_errs, 3),\n",
        "                      np.array([9.82, 8.23, 7.49]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o4RjCMGMGQi"
      },
      "source": [
        "### Use implemented functions to train different Neural Nets and choose the best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WTfmkmoeMGQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc4ca5e-e6cd-4e9d-ca23-18e16af2b112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train err 9.998%, val err 8.930%\n",
            "Epoch 1, train err 8.592%, val err 7.930%\n",
            "Epoch 2, train err 7.186%, val err 6.810%\n",
            "Epoch 3, train err 6.372%, val err 5.830%\n",
            "Epoch 4, train err 5.750%, val err 5.350%\n",
            "Epoch 5, train err 5.404%, val err 5.060%\n",
            "Epoch 6, train err 4.944%, val err 4.810%\n",
            "Epoch 7, train err 4.670%, val err 4.560%\n",
            "Epoch 8, train err 4.386%, val err 4.370%\n",
            "Epoch 9, train err 4.110%, val err 4.210%\n",
            "Epoch 10, train err 3.882%, val err 4.000%\n",
            "Epoch 11, train err 3.678%, val err 3.970%\n",
            "Epoch 12, train err 3.490%, val err 3.840%\n",
            "Epoch 13, train err 3.310%, val err 3.720%\n",
            "Epoch 14, train err 3.210%, val err 3.620%\n",
            "Epoch 15, train err 3.042%, val err 3.590%\n",
            "Epoch 16, train err 2.952%, val err 3.510%\n",
            "Epoch 17, train err 2.818%, val err 3.440%\n",
            "Epoch 18, train err 2.786%, val err 3.450%\n",
            "Epoch 19, train err 2.646%, val err 3.400%\n",
            "Epoch 20, train err 2.596%, val err 3.320%\n",
            "Epoch 21, train err 2.478%, val err 3.270%\n",
            "Epoch 22, train err 2.370%, val err 3.180%\n",
            "Epoch 23, train err 2.394%, val err 3.230%\n",
            "Epoch 24, train err 2.280%, val err 3.130%\n",
            "Epoch 25, train err 2.214%, val err 3.090%\n",
            "Epoch 26, train err 2.230%, val err 3.210%\n",
            "Epoch 27, train err 2.120%, val err 3.080%\n",
            "Epoch 28, train err 2.038%, val err 3.060%\n",
            "Epoch 29, train err 2.026%, val err 2.960%\n"
          ]
        }
      ],
      "source": [
        "# Shallow Neural Net\n",
        "Ws_50, train_errs_50, val_errs_50 = \\\n",
        "    train_nnet(train_X, train_Y, val_X, val_Y, \n",
        "               hid_layer_sizes=[50], \n",
        "               mb_size=32, learning_rate=0.1, max_epoch=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d4QopYLTMGQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa14ce7-22f8-4e89-927d-3e77cf0ba087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train err 15.316%, val err 14.110%\n",
            "Epoch 1, train err 9.948%, val err 9.000%\n",
            "Epoch 2, train err 7.972%, val err 7.390%\n",
            "Epoch 3, train err 6.688%, val err 6.440%\n",
            "Epoch 4, train err 5.838%, val err 5.520%\n",
            "Epoch 5, train err 5.768%, val err 5.270%\n",
            "Epoch 6, train err 4.638%, val err 4.490%\n",
            "Epoch 7, train err 4.384%, val err 4.320%\n",
            "Epoch 8, train err 3.888%, val err 4.060%\n",
            "Epoch 9, train err 3.592%, val err 3.840%\n",
            "Epoch 10, train err 3.350%, val err 3.760%\n",
            "Epoch 11, train err 3.024%, val err 3.490%\n",
            "Epoch 12, train err 2.808%, val err 3.310%\n",
            "Epoch 13, train err 2.626%, val err 3.240%\n",
            "Epoch 14, train err 2.482%, val err 3.300%\n",
            "Epoch 15, train err 2.388%, val err 3.220%\n",
            "Epoch 16, train err 2.292%, val err 3.100%\n",
            "Epoch 17, train err 2.046%, val err 3.000%\n",
            "Epoch 18, train err 2.038%, val err 3.090%\n",
            "Epoch 19, train err 1.912%, val err 2.980%\n",
            "Epoch 20, train err 1.786%, val err 2.750%\n",
            "Epoch 21, train err 1.732%, val err 2.870%\n",
            "Epoch 22, train err 1.722%, val err 3.080%\n",
            "Epoch 23, train err 1.576%, val err 2.870%\n",
            "Epoch 24, train err 1.538%, val err 2.890%\n",
            "Epoch 25, train err 1.396%, val err 2.930%\n",
            "Epoch 26, train err 1.284%, val err 2.810%\n",
            "Epoch 27, train err 1.316%, val err 2.880%\n",
            "Epoch 28, train err 1.234%, val err 2.800%\n",
            "Epoch 29, train err 1.154%, val err 2.830%\n"
          ]
        }
      ],
      "source": [
        "# Deeper Neural Net\n",
        "Ws_50_50, train_errs_50_50, val_errs_50_50 = \\\n",
        "    train_nnet(train_X, train_Y, val_X, val_Y, \n",
        "               hid_layer_sizes=[50, 50], \n",
        "               mb_size=32, learning_rate=0.1, max_epoch=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DQAnDlVBMGQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac67e36-058b-427c-fed9-faa4d421af64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train err 57.134%, val err 56.460%\n",
            "Epoch 1, train err 18.336%, val err 16.730%\n",
            "Epoch 2, train err 10.690%, val err 9.870%\n",
            "Epoch 3, train err 8.028%, val err 7.620%\n",
            "Epoch 4, train err 6.176%, val err 5.960%\n",
            "Epoch 5, train err 5.308%, val err 5.230%\n",
            "Epoch 6, train err 4.412%, val err 4.630%\n",
            "Epoch 7, train err 3.892%, val err 4.280%\n",
            "Epoch 8, train err 3.588%, val err 3.880%\n",
            "Epoch 9, train err 3.230%, val err 3.860%\n",
            "Epoch 10, train err 2.974%, val err 3.810%\n",
            "Epoch 11, train err 2.686%, val err 3.600%\n",
            "Epoch 12, train err 2.740%, val err 3.890%\n",
            "Epoch 13, train err 2.272%, val err 3.400%\n",
            "Epoch 14, train err 2.552%, val err 3.570%\n",
            "Epoch 15, train err 2.530%, val err 3.530%\n",
            "Epoch 16, train err 1.922%, val err 3.420%\n",
            "Epoch 17, train err 1.904%, val err 3.460%\n",
            "Epoch 18, train err 1.762%, val err 3.220%\n",
            "Epoch 19, train err 1.366%, val err 3.040%\n",
            "Epoch 20, train err 1.264%, val err 3.090%\n",
            "Epoch 21, train err 1.394%, val err 3.360%\n",
            "Epoch 22, train err 1.284%, val err 3.190%\n",
            "Epoch 23, train err 1.066%, val err 2.940%\n",
            "Epoch 24, train err 1.110%, val err 3.290%\n",
            "Epoch 25, train err 1.176%, val err 3.330%\n",
            "Epoch 26, train err 0.746%, val err 3.040%\n",
            "Epoch 27, train err 0.970%, val err 3.100%\n",
            "Epoch 28, train err 0.680%, val err 3.110%\n",
            "Epoch 29, train err 0.700%, val err 3.110%\n"
          ]
        }
      ],
      "source": [
        "# Even more deeper Neural Net!\n",
        "Ws_50_50_50, train_errs_50_50_50, val_errs_50_50_50 = \\\n",
        "    train_nnet(train_X, train_Y, val_X, val_Y, \n",
        "               hid_layer_sizes=[50, 50, 50], \n",
        "               mb_size=32, learning_rate=0.1, max_epoch=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg3SSd8NMGQk"
      },
      "source": [
        "Visualize results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "id": "vBPbTAo5MGQk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a17f12bf-4185-4297-ed5c-fac74e714b49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(np.log(train_errs_50), color='blue', \n",
        "         label='Net 50 - train err')\n",
        "plt.plot(np.log(val_errs_50), color='blue', linestyle='--', \n",
        "         label='Net 50 - val err')\n",
        "plt.plot(np.log(train_errs_50_50), color='green',\n",
        "         label='Net 50_50 - train err')\n",
        "plt.plot(np.log(val_errs_50_50), color='green', linestyle='--',\n",
        "         label='Net 50_50 - val err')\n",
        "plt.plot(np.log(train_errs_50_50_50), color='red',\n",
        "         label='Net 50_50_50 - train err')\n",
        "plt.plot(np.log(val_errs_50_50_50), color='red', linestyle='--',\n",
        "         label='Net 50_50_50 - val err')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Ln of error (for clarity)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO7T2uLWMGQk"
      },
      "source": [
        "TODO: comment (you can write in Vietnamese) on the graph above (Why is it like this? In case you don't know why, just simply say so) *(2 points)*.\n",
        "\n",
        "---\n",
        "**Nhận xét**\n",
        "- Nhìn chung, độ lỗi của các neuron nets khi xét trên train set sẽ thấp hơn so với validation set, do chúng ta sử dụng train set để cập nhật các ma trận trọng số nên khi test trên tập này sẽ có độ lỗi thấp hơn.\n",
        "- Đối với các đường biểu diễn độ lỗi trên train set, ta nhận thấy khi chạy càng nhiều epoch thì các đường này ngày càng phân biệt nhau. Hidden layer của các neuron nets trên có cùng số neurons và neuron net có số hidden layer càng nhiều thì tốc độ đạt trạng thái khớp dữ liệu càng nhanh (độ lỗi tiến sát về 0 với số epoch ít hơn).\n",
        "- Đối với các đường biểu diễn độ lỗi trên validation set, các đường này không phân biệt quá nhiều trên trục độ lỗi (mặc dù đã lấy giá trị log). Tuy nhiên nhìn kỹ thì neuron net có 2 hidden layers cho độ lỗi thấp nhất, trong khi neuron net có 3 hidden layers lại tốt nhất trên train set. </br>\n",
        "$\\rightarrow$ Các neuron nets có hiệu suất quá cao trên tập dữ liệu huấn luyện thì lại không tốt trên dữ liệu thực nghiệm (overfitting).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlbB_3xLMGQk"
      },
      "source": [
        "We will choose the Neural Net model having smallest mean binary error on the validation set as our best Neural Net model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr6LJUQrMGQl"
      },
      "source": [
        "## How good our Neural Net actually is\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Li-We5MGQl"
      },
      "source": [
        "Let's compute mean binary error ($\\in [0, 100]$) of our best Neural Net model on the test set and store result in `test_err` variable *(1 points)*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OLOePCn-MGQl"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "test = compute_nnet_outputs(Ws_50_50, test_X, False)\n",
        "test_err = np.mean(np.argmax(test, axis=1) != test_Y) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1Wb9N2g7MGQl"
      },
      "outputs": [],
      "source": [
        "# CHECK CORRECTNESS\n",
        "assert np.round(test_err, 3) == 2.860"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "NeuralNetLab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "103px",
        "width": "252px"
      },
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "509px",
        "left": "0px",
        "right": "1212px",
        "top": "106px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}